/**
 * Copyright or © or Copr. IETR/INSA - Rennes (2013 - 2019) :
 *
 * Julien Hascoet <jhascoet@kalray.eu> (2016 - 2017)
 * Karol Desnos <karol.desnos@insa-rennes.fr> (2013 - 2017)
 *
 * This software is a computer program whose purpose is to help prototyping
 * parallel applications using dataflow formalism.
 *
 * This software is governed by the CeCILL  license under French law and
 * abiding by the rules of distribution of free software.  You can  use,
 * modify and/ or redistribute the software under the terms of the CeCILL
 * license as circulated by CEA, CNRS and INRIA at the following URL
 * "http://www.cecill.info".
 *
 * As a counterpart to the access to the source code and  rights to copy,
 * modify and redistribute granted by the license, users are provided only
 * with a limited warranty  and the software's author,  the holder of the
 * economic rights,  and the successive licensors  have only  limited
 * liability.
 *
 * In this respect, the user's attention is drawn to the risks associated
 * with loading,  using,  modifying and/or developing or reproducing the
 * software by the user in light of its specific status of free software,
 * that may mean  that it is complicated to manipulate,  and  that  also
 * therefore means  that it is reserved for developers  and  experienced
 * professionals having in-depth computer knowledge. Users are therefore
 * encouraged to load and test the software's suitability as regards their
 * requirements in conditions enabling the security of their systems and/or
 * data to be ensured and,  more generally, to use and operate it in the
 * same conditions as regards security.
 *
 * The fact that you are presently reading this means that you have had
 * knowledge of the CeCILL license and that you accept its terms.
 */
package org.preesm.codegen.xtend.printer.c

import java.util.Date
import java.util.List
import org.preesm.codegen.model.Buffer
import org.preesm.codegen.model.CallBlock
import org.preesm.codegen.model.Communication
import org.preesm.codegen.model.Constant
import org.preesm.codegen.model.ConstantString
import org.preesm.codegen.model.CoreBlock
import org.preesm.codegen.model.Delimiter
import org.preesm.codegen.model.Direction
import org.preesm.codegen.model.FifoCall
import org.preesm.codegen.model.FifoOperation
import org.preesm.codegen.model.FiniteLoopBlock
import org.preesm.codegen.model.FunctionCall
import org.preesm.codegen.model.LoopBlock
import org.preesm.codegen.model.NullBuffer
import org.preesm.codegen.model.SharedMemoryCommunication
import org.preesm.codegen.model.SpecialCall
import org.preesm.codegen.model.SubBuffer
import org.preesm.codegen.model.Variable
import org.preesm.commons.exceptions.PreesmRuntimeException
import org.preesm.codegen.model.Block
import java.util.Arrays
import org.preesm.commons.files.URLResolver
import org.preesm.codegen.xtend.CodegenPlugin
import java.io.IOException
import org.apache.velocity.app.VelocityEngine
import org.preesm.model.pisdf.util.CHeaderUsedLocator
import org.apache.velocity.VelocityContext
import java.net.URL
import java.io.InputStreamReader
import java.io.StringWriter
import java.util.Collection

class MPPA2ExplicitPrinter extends CPrinter {

	new() {
		// do not generate a main file
		super(true)
	}

	/**
	 * Temporary global var to ignore the automatic suppression of memcpy
	 * whose target and destination are identical.
	 */
	protected boolean IGNORE_USELESS_MEMCPY = true

	protected String local_buffer = "local_buffer"

	protected boolean IS_HIERARCHICAL = false

	protected String scratch_pad_buffer = ""

	protected long local_buffer_size = 0

	override printCoreBlockHeader(CoreBlock block) '''
		/**
		 * @file «block.name».c
		 * @generated by «this.class.simpleName»
		 * @date «new Date»
		 */

		/* system includes */
		#include <stdlib.h>
		#include <stdio.h>
		#include <stdint.h>
		#include <mOS_vcore_u.h>
		#include <mppa_noc.h>
		#include <mppa_rpc.h>
		#include <mppa_async.h>
		#include <pthread.h>
		#include <semaphore.h>
		#ifndef __nodeos__
		#include <utask.h>
		#endif

		/* user includes */
		#include "preesm.h"

		extern void *__wrap_memset (void *s, int c, size_t n);
		extern void *__wrap_memcpy(void *dest, const void *src, size_t n);
		
		#define memset __wrap_memset
		#define memcpy __wrap_memcpy
		
		/* local Core variables */
		#ifdef PROFILE
		#define DUMP_MAX_TIME 128
		static uint64_t timestamp[NB_CORE][DUMP_MAX_TIME]; /* 4KB of data */
		static int current_dump[NB_CORE] = { 0 };
		#define getTimeProfile() if(current_dump[__k1_get_cpu_id()] < DUMP_MAX_TIME) \
								timestamp[__k1_get_cpu_id()][current_dump[__k1_get_cpu_id()]] = __k1_read_dsu_timestamp(); \
								current_dump[__k1_get_cpu_id()]++;
		#endif

		extern long long total_get_cycles[];
		extern long long total_put_cycles[];
		extern mppa_async_segment_t shared_segment;

		/* Scratchpad buffer ptr (will be malloced) */
		char *local_buffer __attribute__((unused)) = NULL;
		/* Scratchpad buffer size */
		int local_buffer_size __attribute__((unused)) = 0;

	'''

	override printBufferDefinition(Buffer buffer) '''
		«IF buffer.name == "Shared"»
		//#define Shared ((char*)0x10000000ULL) 	/* Shared buffer in DDR */
		«ELSE»
		«buffer.type» «buffer.name»[«buffer.size»] __attribute__ ((aligned(64))); // «buffer.comment» size:= «buffer.size»*«buffer.type» aligned on data cache line
		«ENDIF»
	'''

	override printDefinitionsHeader(List<Variable> list) '''
	«IF !list.empty»
		// Core Global Definitions

	«ENDIF»
	'''

	override printSubBufferDefinition(SubBuffer buffer) '''
	«buffer.type» *const «buffer.name» = («buffer.type»*) («var offset = 0L»«
	{offset = buffer.offset
	 var b = buffer.container;
	 while(b instanceof SubBuffer){
	 	offset = offset + b.offset
	  	b = b.container
	  }
	 b}.name»+«offset»);  // «buffer.comment» size:= «buffer.size»*«buffer.type»
	'''

	override printFiniteLoopBlockHeader(FiniteLoopBlock block2) '''
	«{
	 	IS_HIERARCHICAL = true
	"\t"}»// Begin the for loop
	{
		«{
		var gets = ""
		var local_offset = 0L;
			/* go through eventual out param first because of foot FiniteLoopBlock */
			for(param : block2.outBuffers){
				var b = param.container;
				var offset = param.offset;
				while(b instanceof SubBuffer){
					offset += b.offset;
					b = b.container;
				}
				/* put out buffer here */
				if(b.name == "Shared"){
					gets += "void *" + param.name + " = local_buffer+" + local_offset +";\n";
					local_offset += param.typeSize * param.size;
				}
			}
			for(param : block2.inBuffers){
				var b = param.container;
				var offset = param.offset;
				while(b instanceof SubBuffer){
					offset += b.offset;
					b = b.container;
				}
				//System.out.print("===> " + b.name + "\n");
				if(b.name == "Shared"){
					gets += "void *" + param.name + " = local_buffer+" + local_offset +";\n";
					gets += "{\n"
					gets += "	uint64_t start = __k1_read_dsu_timestamp();\n"
					gets += "	mppa_async_get(local_buffer + " + local_offset + ",\n";
					gets += "	&shared_segment,\n";
					//gets += "	" + b.name + " + " + offset + ",\n";
					gets += "	/* Shared + */ " + offset + ",\n";
					gets += "	" + param.typeSize * param.size + ",\n";
					gets += "	NULL);\n";
					gets += "	__builtin_k1_afdau(&total_get_cycles[__k1_get_cpu_id()], (__k1_read_dsu_timestamp() - start));\n"
					gets += "}\n"
					local_offset += param.typeSize * param.size;
					//System.out.print("==> " + b.name + " " + param.name + " size " + param.size + " port_name "+ port.getName + "\n");
				}
			}

			gets += "int " + block2.iter.name + ";\n"
			gets += "#pragma omp parallel for private(" + block2.iter.name + ")\n"
			gets += "for(" + block2.iter.name + "=0;" + block2.iter.name +"<" + block2.nbIter + ";" + block2.iter.name + "++)\n"
			gets += "	{\n"


			if(local_offset > local_buffer_size)
				local_buffer_size = local_offset
	gets}»
	'''

	override printFiniteLoopBlockFooter(FiniteLoopBlock block2) '''
		}
		«{
				var puts = ""
				var local_offset = 0L;
				for(param : block2.outBuffers){
					var b = param.container
					var offset = param.offset
					while(b instanceof SubBuffer){
						offset += b.offset;
						b = b.container;
						//System.out.print("Running through all buffer " + b.name + "\n");
					}
					//System.out.print("===> " + b.name + "\n");
					if(b.name == "Shared"){
						puts += "{\n"
						puts += "	uint64_t start = __k1_read_dsu_timestamp();\n"
						puts += "	mppa_async_put(local_buffer + " + local_offset + ",\n";
						puts += "	&shared_segment,\n";
						puts += "	/* Shared + */" + offset + ",\n";
						puts += "	" + param.typeSize * param.size + ",\n";
						puts += "	NULL);\n";
						puts += "	__builtin_k1_afdau(&total_put_cycles[__k1_get_cpu_id()], __k1_read_dsu_timestamp() - start);\n"
						puts += "	}\n"
						local_offset += param.typeSize * param.size;
						//System.out.print("==> " + b.name + " " + param.name + " size " + param.size + " port_name "+ port.getName + "\n");
					}
				}
				if(local_offset > local_buffer_size)
					local_buffer_size = local_offset
				puts += "}\n"
			puts}»
		«{
			 	IS_HIERARCHICAL = false
			""}»
	'''

	override printFunctionCall(FunctionCall functionCall) '''
	«{
		var gets = ""
		var local_offset = 0L;
		if(IS_HIERARCHICAL == false){
			gets += "{\n"
			for(param : functionCall.parameters){

				if(param instanceof SubBuffer){
					var port = functionCall.parameterDirections.get(functionCall.parameters.indexOf(param))
					var b = param.container;
					var offset = param.offset;
					while(b instanceof SubBuffer){
						offset += b.offset;
						b = b.container;
						//System.out.print("Running through all buffer " + b.name + "\n");
					}
					//System.out.print("===> " + b.name + "\n");
					if(b.name == "Shared"){
						gets += "	void *" + param.name + " = local_buffer+" + local_offset +";\n";
						if(port.getName == "INPUT"){ /* we get data from DDR -> cluster only when INPUT */
							gets += "	{\n"
							gets += "		uint64_t start = __k1_read_dsu_timestamp();\n"
							gets += "		mppa_async_get(local_buffer+" + local_offset + ", &shared_segment, /* Shared + */ " + offset + ", " + param.typeSize * param.size + ", NULL);\n";
							gets += "		__builtin_k1_afdau(&total_get_cycles[__k1_get_cpu_id()], __k1_read_dsu_timestamp() - start);\n"
							gets += "	}\n"
						}
						local_offset += param.typeSize * param.size;
						//System.out.print("==> " + b.name + " " + param.name + " size " + param.size + " port_name "+ port.getName + "\n");
					}
				}
			}
			gets += "\t"
		}else{
			gets += " /* gets are normaly generated before */ \n"
		}
		if(local_offset > local_buffer_size)
			local_buffer_size = local_offset
	gets}»
		«functionCall.name»(«FOR param : functionCall.parameters SEPARATOR ', '»«param.doSwitch»«ENDFOR»); // «functionCall.actorName»
	«{
		var puts = ""
		var local_offset = 0L;
		if(IS_HIERARCHICAL == false){
			for(param : functionCall.parameters){
				if(param instanceof SubBuffer){
					var port = functionCall.parameterDirections.get(functionCall.parameters.indexOf(param))
					var b = param.container
					var offset = param.offset
					while(b instanceof SubBuffer){
						offset += b.offset;
						b = b.container;
						//System.out.print("Running through all buffer " + b.name + "\n");
					}
					//System.out.print("===> " + b.name + "\n");
					if(b.name == "Shared"){
						if(port.getName == "OUTPUT"){ /* we put data from cluster -> DDR only when OUTPUT */
							puts += "	{\n"
							puts += "		uint64_t start = __k1_read_dsu_timestamp();\n"
							puts += "		mppa_async_put(local_buffer+" + local_offset + ", &shared_segment, /* Shared + */ " + offset + ", " + param.typeSize * param.size + ", NULL);\n";
							puts += "		__builtin_k1_afdau(&total_put_cycles[__k1_get_cpu_id()], __k1_read_dsu_timestamp() - start);\n"
							puts += "	}\n"
						}
						local_offset += param.typeSize * param.size;
						//System.out.print("==> " + b.name + " " + param.name + " size " + param.size + " port_name "+ port.getName + "\n");
					}
				}
			}
			puts += "}\n"
		}else{
			puts += " /* puts are normaly generated before */ \n"
		}
		if(local_offset > local_buffer_size)
			local_buffer_size = local_offset
	puts}»
	'''

	override printDefinitionsFooter(List<Variable> list) '''
	«IF !list.empty»

	«ENDIF»
	'''

	override printDeclarationsHeader(List<Variable> list) '''
	// Core Global Declaration
	extern pthread_barrier_t pthread_barrier;

	'''

	override printBufferDeclaration(Buffer buffer) '''
	extern «printBufferDefinition(buffer)»
	'''

	override printSubBufferDeclaration(SubBuffer buffer) '''
	«buffer.type» *const «buffer.name» = («buffer.type»*) («var offset = 0L»«
	{offset = buffer.offset
	 var b = buffer.container;
	 while(b instanceof SubBuffer){
	 	offset = offset + b.offset
	  	b = b.container
	  }
	 b}.name»+«offset»);  // «buffer.comment» size:= «buffer.size»*«buffer.type»
	'''

	override printDeclarationsFooter(List<Variable> list) '''
	«IF !list.empty»

	«ENDIF»
	'''

	override printCoreInitBlockHeader(CallBlock callBlock) '''
	void *computationTask_«(callBlock.eContainer as CoreBlock).name»(void *arg){
	#ifdef PREESM_VERBOSE
		//printf("Cluster %d runs on task «(callBlock.eContainer as CoreBlock).name»\n", __k1_get_cluster_id());
	#endif
		«IF !callBlock.codeElts.empty»
			// Initialisation(s)

		«ENDIF»
	'''

	override printCoreLoopBlockHeader(LoopBlock block2) '''

		«"\t"»// Begin the execution loop
		#ifdef PREESM_LOOP_SIZE // Case of a finite loop
			int __iii __attribute__((unused));
			for(__iii=0;__iii<PREESM_LOOP_SIZE;__iii++){
		#else // Default case of an infinite loop
			while(1){
		#endif

				//pthread_barrier_wait(&pthread_barrier);
		#ifdef PROFILE
				getTimeProfile();
		#endif

	'''


	override printCoreLoopBlockFooter(LoopBlock block2) '''
		#ifdef PREESM_VERBOSE
				mppa_rpc_barrier_all(); /* sync all PE0 of all Clusters */
				if(__k1_get_cpu_id() == 0 && __k1_get_cluster_id() == 0){
					//printf("C0->%d Graph Iteration %d / %d Done !\n", NB_CLUSTER, __iii+1, PREESM_LOOP_SIZE);
				}
				mppa_rpc_barrier_all(); /* sync all PE0 of all Clusters */
		#endif
				/* commit local changes to the global memory */
				//pthread_barrier_wait(&pthread_barrier); /* barrier to make sure all threads have commited data in smem */
				if(__k1_get_cpu_id() == 0){
		#ifdef PROFILE
					int iii;
					for(iii=0;iii<__k1_get_cluster_id();iii++)
						mppa_rpc_barrier_all();
					int ii, jj;
					for(jj=0;jj<NB_CORE;jj++){
						if(current_dump[jj] != 0){
							printf("C%d PE%d : Number of actors %d\n", __k1_get_cluster_id(), jj, current_dump[jj]);
							printf("\t# Profile %d Timestamp %lld\n", 0, (long long)timestamp[jj][0]);
							for(ii=1;ii<current_dump[jj];ii++){
								printf("\t# C%d Profile %d Timestamp %lld Cycle %lld Time %.4f ms\n",
										__k1_get_cluster_id(),
										ii,
										(long long)timestamp[jj][ii],
										(long long)timestamp[jj][ii]-(long long)timestamp[jj][ii-1],
										((float)timestamp[jj][ii]-(float)timestamp[jj][ii-1])/400000.0f /* chip freq */);
							}
						}
					}
					for(iii=__k1_get_cluster_id();iii<NB_CLUSTER;iii++)
						mppa_rpc_barrier_all();
		#endif
				}

			}
			return NULL;
		}
	'''
	override printFifoCall(FifoCall fifoCall) {
		var result = "fifo" + fifoCall.operation.toString.toLowerCase.toFirstUpper + "("

		if (fifoCall.operation != FifoOperation::INIT) {
			var buffer = fifoCall.parameters.head as Buffer
			result = result + '''«buffer.doSwitch», '''
		}

		result = result +
			'''«fifoCall.headBuffer.name», «fifoCall.headBuffer.size»*sizeof(«fifoCall.headBuffer.type»), '''
		result = result + '''«IF fifoCall.bodyBuffer !== null»«fifoCall.bodyBuffer.name», «fifoCall.bodyBuffer.size»*sizeof(«fifoCall.
			bodyBuffer.type»)«ELSE»NULL, 0«ENDIF»);
			'''

		return result
	}

	override printFork(SpecialCall call) '''
	// Fork «call.name»«var input = call.inputBuffers.head»«var index = 0L»
	{
		«FOR output : call.outputBuffers»
			«printMemcpy(output,0,input,index,output.size,output.type)»«{index=(output.size+index); ""}»
		«ENDFOR»
	}
	'''

	override printBroadcast(SpecialCall call) '''
		«{
			super.printBroadcast(call)
		}»
	'''

	override printRoundBuffer(SpecialCall call) '''
		«{
			super.printRoundBuffer(call)
		}»
	'''

	override printJoin(SpecialCall call) '''
	// Join «call.name»«var output = call.outputBuffers.head»«var index = 0L»
	{
		«FOR input : call.inputBuffers»
			«printMemcpy(output,index,input,0,input.size,input.type)»«{index=(input.size+index); ""}»
		«ENDFOR»
	}
	'''

	/**
	 * Print a memcpy call in the generated code. Unless
	 * {@link #IGNORE_USELESS_MEMCPY} is set to <code>true</code>, this method
	 * checks if the destination and the source of the memcpy are superimposed.
	 * In such case, the memcpy is useless and nothing is printed.
	 *
	 * @param output
	 *            the destination {@link Buffer}
	 * @param outOffset
	 *            the offset in the destination {@link Buffer}
	 * @param input
	 *            the source {@link Buffer}
	 * @param inOffset
	 *            the offset in the source {@link Buffer}
	 * @param size
	 *            the amount of memory to copy
	 * @param type
	 *            the type of objects copied
	 * @return a {@link CharSequence} containing the memcpy call (if any)
	 */
	override printMemcpy(Buffer output, long outOffset, Buffer input, long inOffset, long size, String type) {

		// Retrieve the container buffer of the input and output as well
		// as their offset in this buffer
		var totalOffsetOut = outOffset*output.typeSize
		var bOutput = output
		while (bOutput instanceof SubBuffer) {
			totalOffsetOut = totalOffsetOut + bOutput.offset
			bOutput = bOutput.container
		}

		var totalOffsetIn = inOffset*input.typeSize
		var bInput = input
		while (bInput instanceof SubBuffer) {
			totalOffsetIn = totalOffsetIn + bInput.offset
			bInput = bInput.container
		}

		// If the Buffer and offsets are identical, or one buffer is null
		// there is nothing to print
		if((IGNORE_USELESS_MEMCPY && bInput == bOutput && totalOffsetIn == totalOffsetOut) ||
			output instanceof NullBuffer || input instanceof NullBuffer){
			return ""
		} else {
			return '''memcpy(«output.doSwitch»+«outOffset», «input.doSwitch»+«inOffset», «size»*sizeof(«type»));'''
		}
	}

	override printNullBuffer(NullBuffer Buffer) {
		return printBuffer(Buffer)
	}

	override caseCommunication(Communication communication) {

		if(communication.nodes.forall[type == "SHARED_MEM"]) {
			return super.caseCommunication(communication)
		} else {
			throw new PreesmRuntimeException("Communication "+ communication.name +
				 " has at least one unsupported communication node"+
				 " for the " + this.class.name + " printer")
		}
	}

	override printSharedMemoryCommunication(SharedMemoryCommunication communication) '''
		«communication.direction.toString.toLowerCase»«communication.delimiter.toString.toLowerCase.toFirstUpper»(«IF (communication.
			direction == Direction::SEND && communication.delimiter == Delimiter::START) ||
			(communication.direction == Direction::RECEIVE && communication.delimiter == Delimiter::END)»«{
			var coreName = if (communication.direction == Direction::SEND) {
					communication.receiveStart.coreContainer.name
				} else {
					communication.sendStart.coreContainer.name
				}
			var ret = coreName.substring(4, coreName.length)
			ret
		}»«ENDIF»); // «communication.sendStart.coreContainer.name» > «communication.receiveStart.coreContainer.name»: «communication.
			data.doSwitch»
	'''

	override printConstant(Constant constant) '''«constant.value»«IF !constant.name.nullOrEmpty»/*«constant.name»*/«ENDIF»'''

	override printConstantString(ConstantString constant) '''"«constant.value»"'''

	override printBuffer(Buffer buffer) '''«buffer.name»'''

	override printSubBuffer(SubBuffer buffer) {
		return printBuffer(buffer)
	}
	
	override CharSequence generatePreesmHeader() {
	    // 0- without the following class loader initialization, I get the following exception when running as Eclipse
	    // plugin:
	    // org.apache.velocity.exception.VelocityException: The specified class for ResourceManager
	    // (org.apache.velocity.runtime.resource.ResourceManagerImpl) does not implement
	    // org.apache.velocity.runtime.resource.ResourceManager; Velocity is not initialized correctly.
	    val ClassLoader oldContextClassLoader = Thread.currentThread().getContextClassLoader();
	    Thread.currentThread().setContextClassLoader(CPrinter.classLoader);

	    // 1- init engine
	    val VelocityEngine engine = new VelocityEngine();
	    engine.init();

	    // 2- init context
	    val VelocityContext context = new VelocityContext();
	    val findAllCHeaderFileNamesUsed = CHeaderUsedLocator.findAllCHeaderFileNamesUsed(getEngine.algo.referencePiMMGraph)
	    context.put("USER_INCLUDES", findAllCHeaderFileNamesUsed.map["#include \""+ it +"\""].join("\n"));


	    context.put("CONSTANTS", "#define NB_DESIGN_ELTS "+getEngine.archi.componentInstances.size+"\n#define NB_CLUSTERS "+getEngine.codeBlocks.size);

	    // 3- init template reader
	    val String templateLocalURL = "templates/c/preesm_gen.h";
	    val URL mainTemplate = URLResolver.findFirstInBundleList(templateLocalURL, CodegenPlugin.BUNDLE_ID);
	    var InputStreamReader reader = null;
	    try {
	      reader = new InputStreamReader(mainTemplate.openStream());
	    } catch (IOException e) {
	      throw new PreesmRuntimeException("Could not locate main template [" + templateLocalURL + "].", e);
	    }

	    // 4- init output writer
	    val StringWriter writer = new StringWriter();

	    engine.evaluate(context, writer, "org.apache.velocity", reader);

	    // 99- set back default class loader
	    Thread.currentThread().setContextClassLoader(oldContextClassLoader);

	    return writer.getBuffer().toString();
	}
	override generateStandardLibFiles() {
		val result = super.generateStandardLibFiles();
		val String stdFilesFolder = "/stdfiles/mppa2Explicit/"
		val files = Arrays.asList(#[
						"communication.c",
						"communication.h",
						"dump.c",
						"dump.h",
						"fifo.c",
						"fifo.h",
						"memory.c",
						"memory.h",
						"clock.c",
						"clock.h"
					]);
		files.forEach[it | try {
			result.put(it, URLResolver.readURLInBundleList(stdFilesFolder + it, CodegenPlugin.BUNDLE_ID))
		} catch (IOException exc) {
			throw new PreesmRuntimeException("Could not generated content for " + it, exc)
		}]
		result.put("preesm_gen.h",generatePreesmHeader())
		return result
	}
	override createSecondaryFiles(List<Block> printerBlocks, Collection<Block> allBlocks) {
		val result = super.createSecondaryFiles(printerBlocks, allBlocks);
		result.clear();
		if (generateMainFile()) {
			result.put("cluster_main.c", printMainCluster(printerBlocks));
			result.put("io_main.c", printMainIO(printerBlocks));
		}
		return result
	}
	def String printMainCluster(List<Block> printerBlocks) '''
		/**
		 * @file cluster_main.c
		 * @generated by «this.class.simpleName»
		 * @date «new Date»
		 *
		 */
		/*
		 * Copyright (C) 2016 Kalray SA.
		 *
		 * All rights reserved.
		 */
		#include "mOS_common_types_c.h"
		#include "mOS_constants_c.h"
		#include "mOS_vcore_u.h"
		#include "mOS_segment_manager_u.h"
		#include "stdlib.h"
		#include "stdio.h"
		#include "vbsp.h"
		#include <mppa_rpc.h>
		#include <mppa_remote.h>
		#include <mppa_async.h>
		#include "HAL/hal/hal_ext.h"
		#include <math.h>
		#include <stdlib.h>
		
		#ifdef __nodeos__
		#define CONFIGURE_DEFAULT_TASK_STACK_SIZE (1U<<12)
		#define CONFIGURE_AMP_MAIN_STACK_SIZE (1U<<12)
		#include <mppa/osconfig.h>
		#include <omp.h>
		#else
		#include <utask.h>
		#endif
		
		#include <pthread.h>
		
		#include <assert.h>
		
		#include "preesm.h"
		#include "communication.h"
		
		/* Shared Segment ID */
		mppa_async_segment_t shared_segment;
		
		/* MPPA PREESM Thread definition */
		typedef void* (*mppa_preesm_task_t)(void *args);
		
		/* pthread_t declaration */
		static pthread_t threads[NB_CORE-1] __attribute__((__unused__));
		
		/* thread function pointers declaration */
		static mppa_preesm_task_t mppa_preesm_task[NB_CLUSTER] __attribute__((__unused__)); 
		
		/* global barrier called at each execution of ALL of the dataflow graph */
		pthread_barrier_t pthread_barrier __attribute__((__unused__)); 
		
		/* extern reference of generated code */
		extern void *computationTask_Core00(void *arg) __attribute__((__unused__, weak));
		extern void *computationTask_Core01(void *arg) __attribute__((__unused__, weak));
		extern void *computationTask_Core02(void *arg) __attribute__((__unused__, weak));
		extern void *computationTask_Core03(void *arg) __attribute__((__unused__, weak));
		extern void *computationTask_Core04(void *arg) __attribute__((__unused__, weak));
		extern void *computationTask_Core05(void *arg) __attribute__((__unused__, weak));
		extern void *computationTask_Core06(void *arg) __attribute__((__unused__, weak));
		extern void *computationTask_Core07(void *arg) __attribute__((__unused__, weak));
		extern void *computationTask_Core08(void *arg) __attribute__((__unused__, weak));
		extern void *computationTask_Core09(void *arg) __attribute__((__unused__, weak));
		extern void *computationTask_Core10(void *arg) __attribute__((__unused__, weak));
		extern void *computationTask_Core11(void *arg) __attribute__((__unused__, weak));
		extern void *computationTask_Core12(void *arg) __attribute__((__unused__, weak));
		extern void *computationTask_Core13(void *arg) __attribute__((__unused__, weak));
		extern void *computationTask_Core14(void *arg) __attribute__((__unused__, weak));
		extern void *computationTask_Core15(void *arg) __attribute__((__unused__, weak));
		
		/* Total number of cycles spent in async put/get */
		long long total_get_cycles[NB_OMP_CORE] = {0};
		long long total_put_cycles[NB_OMP_CORE] = {0};
		
		/*extern char *Core0 __attribute__((__unused__, weak));
		extern char *Core0 __attribute__((__unused__, weak));
		extern char *Core0 __attribute__((__unused__, weak));
		extern char *Core0 __attribute__((__unused__, weak));
		extern char *Core0 __attribute__((__unused__, weak));
		extern char *Core0 __attribute__((__unused__, weak));*/
		
		/* Main executed on PE0 */
		int
		main(void)
		{
			mppa_rpc_client_init();
			mppa_async_init();
			mppa_remote_client_init();
		
			mppa_async_segment_clone(&shared_segment, SHARED_SEGMENT_ID, NULL, 0, NULL);
		
			//mppa_async_segment_create(&seg, id, Cor)
		
			// init comm
			communicationInit();
		
		#ifdef VERBOSE
			printf("Hello cluster %d\n", __k1_get_cluster_id());
		#endif
			/* Dirty generated threads wrapper to function pointers */ 
		#if (CLUSTER_ID==0)
			mppa_preesm_task[0] = computationTask_Core00;
		#else
		#if (CLUSTER_ID==1)
			mppa_preesm_task[1] = computationTask_Core01;
		#else
		#if (CLUSTER_ID==2)
			mppa_preesm_task[2] = computationTask_Core02;
		#else
		#if (CLUSTER_ID==3)
			mppa_preesm_task[3] = computationTask_Core03;
		#else
		#if (CLUSTER_ID==4)
			mppa_preesm_task[4] = computationTask_Core04;
		#else
		#if (CLUSTER_ID==5)
			mppa_preesm_task[5] = computationTask_Core05;
		#else
		#if (CLUSTER_ID==6)
			mppa_preesm_task[6] = computationTask_Core06;
		#else
		#if (CLUSTER_ID==7)
			mppa_preesm_task[7] = computationTask_Core07;
		#else
		#if (CLUSTER_ID==8)
			mppa_preesm_task[8] = computationTask_Core08;
		#else
		#if (CLUSTER_ID==9)
			mppa_preesm_task[9] = computationTask_Core09;
		#else
		#if (CLUSTER_ID==10)
			mppa_preesm_task[10] = computationTask_Core10;
		#else
		#if (CLUSTER_ID==11)
			mppa_preesm_task[11] = computationTask_Core11;
		#else
		#if (CLUSTER_ID==12)
			mppa_preesm_task[12] = computationTask_Core12;
		#else
		#if (CLUSTER_ID==13)
			mppa_preesm_task[13] = computationTask_Core13;
		#else
		#if (CLUSTER_ID==14)
			mppa_preesm_task[14] = computationTask_Core14;
		#else
		#if (CLUSTER_ID==15)
			mppa_preesm_task[15] = computationTask_Core15;
		#endif // CLUSTER 15
		#endif // CLUSTER 14
		#endif // CLUSTER 13
		#endif // CLUSTER 12
		#endif // CLUSTER 11
		#endif // CLUSTER 10
		#endif // CLUSTER 9
		#endif // CLUSTER 8
		#endif // CLUSTER 7
		#endif // CLUSTER 6
		#endif // CLUSTER 5
		#endif // CLUSTER 4
		#endif // CLUSTER 3
		#endif // CLUSTER 2
		#endif // CLUSTER 1
		#endif // CLUSTER 0
		
		#ifdef VERBOSE
			printf("Cluster %d Booted with NB_CLUSTER %d\n", __k1_get_cluster_id(), NB_CLUSTER);
		#if 0
			{
				int i;
				for (i = 0; i < NB_CLUSTER; i++) {
					printf("Cluster %d PE %d Thread Address %x\n", __k1_get_cluster_id(), __k1_get_cpu_id(), mppa_preesm_task[i]);
				}
			}
		#endif
		#endif
		
			pthread_barrier_init(&pthread_barrier, NULL, NB_CORE);
			__builtin_k1_wpurge();
			__builtin_k1_fence();
			mOS_dinval();
		
		#if 0	
			/* create threads if any */
			for (i = 0; i < NB_CLUSTER-1; i++) {
				printf("Start thread %d\n", i);
				pthread_create(&threads[i], NULL, mppa_preesm_task[i+1], NULL);
			}
		#endif
		
			mppa_rpc_barrier_all();
			uint64_t start = __k1_read_dsu_timestamp(); /* read clock cycle */
		
			/* PE0 work */	
			if(mppa_preesm_task[__k1_get_cluster_id()] != 0){
				//printf("Cluster %d starts task\n", __k1_get_cluster_id());
				mppa_preesm_task[__k1_get_cluster_id()](NULL);
			}else{
				printf("Cluster %d Error on code generator wrapper\n", __k1_get_cluster_id());
			}
		#if 0
			/* join other threads if any */
			for (i = 0; i < NB_CLUSTER-1; i++) {
				printf("Join thread %d\n", i);
				pthread_join(threads[i], NULL);
			}
		#endif
			mppa_rpc_barrier_all();
			uint64_t end = __k1_read_dsu_timestamp(); /* read clock cycle */
		
		#ifdef VERBOSE
			mOS_dinval();
			long long max_total_get_cycles = 0;
			long long max_total_put_cycles = 0;
			for(int i=0;i<NB_OMP_CORE;i++)
			{
				if(max_total_get_cycles < total_get_cycles[i])
					max_total_get_cycles = total_get_cycles[i];
				if(max_total_put_cycles < total_put_cycles[i])
					max_total_put_cycles = total_put_cycles[i];
			}
			printf("Cluster %d Cycles: Total %llu Get %llu Put %llu GetPut %llu Ratio Communication/Total %.2f %%\n", __k1_get_cluster_id(),
				end - start,
				max_total_get_cycles,
				max_total_put_cycles,
				max_total_get_cycles+max_total_put_cycles,
				(float)(max_total_get_cycles+max_total_put_cycles) / (float)(end-start) * 100.0f);
				
				printf("Cluster %d Total %.3f ms Graph Step %.3f ms FPS %.2f\n", 
				__k1_get_cluster_id(), 
				((double)(end - start))/((float)(__bsp_frequency/1000)),
				((double)(end - start))/((float)(__bsp_frequency/1000))/PREESM_LOOP_SIZE, 
				1/((((double)(end - start))/((float)(__bsp_frequency/1000))/PREESM_LOOP_SIZE)/1000.0f));
		#endif
		
			mppa_rpc_barrier_all();
			#ifdef VERBOSE
			if(__k1_get_cluster_id() == 0)
			{
				float bw = (float)((float)(max_total_get_cycles+max_total_put_cycles) / ((float)(end-start)) * 100.0f);
				float fps = 1/((((double)(end - start))/((float)(__bsp_frequency/1000))/PREESM_LOOP_SIZE)/1000.0f);
				float time_ms = ((double)(end - start))/((float)(__bsp_frequency/1000));
				
				#ifdef __nodeos__
				printf("\n\t NB_CLUSTER %d NB_OMP_CORE %d FPS %.2f BW %.2f %% Total run time %.2f ms\n\n", NB_CLUSTER, NB_OMP_CORE, fps, bw, time_ms);
				#else
				printf("\n\t NB_CLUSTER %d FPS %.2f BW %.2f %% Total run time %.2f ms\n\n", NB_CLUSTER, fps, bw, time_ms);
				#endif
			}
			#endif
			mppa_rpc_barrier_all();
			mppa_async_final();
			return 0;
		}

	'''

	def String printMainIO(List<Block> printerBlocks) '''
		/**
		 * @file io_main.c
		 * @generated by «this.class.simpleName»
		 * @date «new Date»
		 *
		 */
		/*
		 * Copyright (C) 2016 Kalray SA.
		 *
		 * All rights reserved.
		 */
		
		#include <stdio.h>
		#include <stdlib.h>
		#include "mppa_boot_args.h"
		#include <mppa_power.h>
		#include <assert.h>
		#include "mppa_bsp.h"
		#include <utask.h>
		#include <pcie_queue.h>
		#include <mppa_rpc.h>
		#include <mppa_remote.h>
		#include <mppa_async.h>
		#include <HAL/hal/board/boot_args.h>
		#include <cluster/preesm.h>
		
		static utask_t t;
		static mppadesc_t pcie_fd = 0;
		
		int
		main(int argc __attribute__ ((unused)), char *argv[] __attribute__ ((unused)))
		{
			int id;
			int j;
			int ret ;
		
			if(__k1_spawn_type() == __MPPA_PCI_SPAWN){
				#if 1
				long long *ptr = (void*)(uintptr_t)Shared;
				long long i;
				for(i=0;i<(long long)((1<<30ULL)/sizeof(long long));i++)
				{
					ptr[i] = -1LL;
				}
				__builtin_k1_wpurge();
				__builtin_k1_fence();
				mOS_dinval();
				#endif
			}
		
			if (__k1_spawn_type() == __MPPA_PCI_SPAWN) {
				pcie_fd = pcie_open(0);
					ret = pcie_queue_init(pcie_fd);
					pcie_register_console(pcie_fd, stdin, stdout);
				assert(ret == 0);
			}
		
		#ifdef VERBOSE	
			printf("Hello IO\n");
		#endif
		
			mppa_rpc_server_init(	1 /* rm where to run server */, 
									0 /* offset ddr */, 
									NB_CLUSTER /* nb_cluster to serve*/);
			mppa_async_server_init();
			mppa_remote_server_init(pcie_fd, NB_CLUSTER);
			
			
			for( j = 0 ; j < NB_CLUSTER ; j++ ) {
		
				char elf_name[30];
				sprintf(elf_name, "cluster%d_bin", j);
		#ifdef VERBOSE
				printf("Load cluster %d with elf %s\n", j, elf_name);
		#endif
				id = mppa_power_base_spawn(j, elf_name, NULL, NULL, MPPA_POWER_SHUFFLING_ENABLED);
				if (id < 0)
					return -2;
			}
		
			utask_create(&t, NULL, (void*)mppa_rpc_server_start, NULL);
		
			mppa_async_segment_t shared_segment;
			mppa_async_segment_create(&shared_segment, SHARED_SEGMENT_ID, (void*)(uintptr_t)Shared, 1024*1024*1024, 0, 0, NULL);
		
		#ifdef VERBOSE
			printf("Waiting for cluster exit \n");
		#endif
		
			int err;
			for( j = 0 ; j < NB_CLUSTER ; j++ ) {
			    mppa_power_base_waitpid (j, &err, 0);
			}
		
			if (__k1_spawn_type() == __MPPA_PCI_SPAWN) {
				pcie_queue_barrier(pcie_fd, 0, &ret);
				pcie_queue_exit(pcie_fd, ret, NULL);
			}
			return 0;
		}

	'''
	override postProcessing(CharSequence charSequence){
		var ret = charSequence.toString.replace("int local_buffer_size __attribute__((unused)) = 0;", "int local_buffer_size __attribute__((unused)) = " + local_buffer_size + ";");
		return ret;
	}
}
